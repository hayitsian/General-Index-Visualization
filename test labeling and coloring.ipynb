{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Index Labeling Test\n",
    "Small scale testing of general index data & processing\n",
    "\n",
    "---\n",
    "Created 6/14/22 by Ian Hay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Dependencies\n",
    "\n",
    "---\n",
    "[Python 3.8+](https://www.python.org/downloads/release/python-380/)    \n",
    "[Pandas](https://pandas.pydata.org/)   \n",
    "[NumPy](https://numpy.org/)    \n",
    "[NetworkX](https://networkx.org/)  \n",
    "[pyvis](https://pyvis.readthedocs.io/en/latest/install.html)   \n",
    "[scikit-learn](https://scikit-learn.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pyvis.network import Network\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing ------------------------------------------------------------------------------------------------\n",
    "\n",
    "def getNounsAndVerbs(df, column, newColumnName):\n",
    "    \"\"\"\n",
    "    Utilizes Spacy to extract nouns and verbs from ngrams\n",
    "    and build a new column with only these terms.\n",
    "    \"\"\"\n",
    "    nounAndVerbDict = {}\n",
    "    for row in range(len(df)):\n",
    "        text = \" \".join(df.iloc[row][column])\n",
    "        doc = nlp(text)\n",
    "        nounList = [chunk.text for chunk in doc.noun_chunks]\n",
    "        verbList = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "        nounAndVerbDict[df.index[row]] = nounList + verbList\n",
    "    dfNounAndVerb = pd.Series(nounAndVerbDict, name=newColumnName)\n",
    "    df[newColumnName] = dfNounAndVerb\n",
    "\n",
    "def standardizeDataColumn(df, column, newColumnName):\n",
    "    \"\"\"\n",
    "    Standardizes the column of the dataframe df.\n",
    "    Adds the new column newColumnName to the dataframe inplace.\n",
    "    Utilizes SKLearn.preprocessing.standardscaler.\n",
    "    Mean is 0, variance is 1\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaledSeries = pd.Series(np.reshape(scaler.fit_transform(np.array(df[column]).reshape(-1, 1)), (-1)))\n",
    "    for n in range(len(df)):\n",
    "        df[newColumnName][n] = scaledSeries[n] + 1 # scales most (~98%) to be in range [0, 2] for graphing purposes\n",
    "\n",
    "def getUniqueWordsColumn(df, column, newColumnName, nonWords=[]):\n",
    "    \"\"\"\n",
    "    Given a dataframe and column, constructs a new column with name newColumnName\n",
    "    of the unique words in  df[column].\n",
    "    The object in  df[column]  must be a list of strings.\n",
    "    Returns the updated dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ### deprecated ###\n",
    "\n",
    "    df[newColumnName] = df[column]\n",
    "    for row in range(len(df[newColumnName])):\n",
    "        df[newColumnName][row] = df[column].iloc[row]\n",
    "        string_list = []\n",
    "        for string in df[newColumnName].iloc[row]:\n",
    "            string_list.append(string.split(\" \")) # splits words into list of individual word strings\n",
    "        string_list = list(itertools.chain(*string_list)) # concatenates nested list into 1D list\n",
    "        string_list = list(set(string_list)) # grabs only unique string items\n",
    "        for nonword in nonWords:\n",
    "            if nonword in string_list:\n",
    "                string_list.remove(nonword)\n",
    "        df[newColumnName].iloc[row] = string_list\n",
    "    return df\n",
    "\n",
    "def buildAdjacencyMatrixByColumn(df, column):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a column, constructs an adjacency matrix\n",
    "    of size [n x n] where  n  is the number of rows of the dataframe.\n",
    "    The adjacency matrix edge weights represent the number of similar elements.\n",
    "    The datatype in  df[column]  must be a list.\n",
    "    \"\"\"\n",
    "    n = len(df[column])\n",
    "    adjMatrix = np.zeros((n, n))\n",
    "    for n1 in range(n):\n",
    "        ngram1 = df[column].iloc[n1]\n",
    "        for n2 in range(n):\n",
    "            ngram2 = df[column].iloc[n2]\n",
    "            numSimilar = numSimilarStrings(ngram1, ngram2)\n",
    "            if n1 != n2 & numSimilar > 0: # removes recursive edges\n",
    "                adjMatrix[n1][n2] = numSimilar \n",
    "    return adjMatrix\n",
    "\n",
    "def buildAdjacencyListByColumn(df, column):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a column, constructs an adjacency list\n",
    "    as a nestd dictionary with  n  keys in the outermost dict, where\n",
    "    n  is the number of rows in the dataframe.\n",
    "    The adjacency list edge weights represent the number of similar elements.\n",
    "    The datatype in  df[column]  must be a list.\n",
    "    \"\"\"\n",
    "    n = len(df[column])\n",
    "\n",
    "    adjDict = {} # consider using the hash to represent nodes instead of numbers\n",
    "    for n1 in range(n):\n",
    "\n",
    "        nodeDict = {}\n",
    "        ngram1 = df[column].iloc[n1]\n",
    "        for n2 in range(n):\n",
    "            ngram2 = df[column].iloc[n2]\n",
    "            numSimilar = numSimilarStrings(ngram1, ngram2)\n",
    "            if n1 != n2 & numSimilar > 0: # removes recursive edges\n",
    "                nodeDict[n2] = {\"weight\" : numSimilar} # https://networkx.org/documentation/stable/reference/generated/networkx.convert.from_dict_of_dicts.html\n",
    "        adjDict[n1] = nodeDict\n",
    "\n",
    "    return adjDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def loadTextFileIntoDataframe(filepath, splittingChar=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Opens the given filepath into a pandas dataframe.\n",
    "    Splits the list by the denoted character, by default tab.\n",
    "    Returns a pandas dataframe.\n",
    "    \"\"\"\n",
    "    with open(filepath) as file:\n",
    "        data = file.readlines()\n",
    "    df = pd.DataFrame()\n",
    "    for line in data:\n",
    "        lineSplit = line.split(splittingChar)\n",
    "        df = df.append([lineSplit]) # pandas gives a warning for this\n",
    "    return df\n",
    "\n",
    "def numSimilarStrings(stringList1, stringList2):\n",
    "    \"\"\"\n",
    "    Given two lists of strings, returns the number of strings they both share.\n",
    "    In other words, the size of the subset intersection of stringList1 and stringList2.\n",
    "    \"\"\"\n",
    "\n",
    "    # is there a faster way to do this with sets?\n",
    "\n",
    "    count = 0\n",
    "    for string in stringList1:\n",
    "        if string in stringList2:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def subtractListsOfInts(_list1, _list2):\n",
    "    \"\"\"\n",
    "    Given two lists of items, returns a list of items\n",
    "    in _list1 and not in _list2.\n",
    "    Utilizes collections.Counter\n",
    "    Returns a list of items.\n",
    "\n",
    "    https://stackoverflow.com/questions/2070643/subtracting-two-lists-in-python\n",
    "    \"\"\"\n",
    "    _set1 = Counter(_list1)\n",
    "    _set2 = Counter(_list2)\n",
    "    _set1_2 = _set1 - _set2\n",
    "    return list(_set1_2.elements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization -------------------------------------------------------------------------------------------\n",
    "\n",
    "def visualizeNetworkHTML(_graph, _filename, _width=\"1920px\", _height=\"1080px\", _physics=False):\n",
    "    \"\"\"\n",
    "    Given a NetworkX graph and the filename to save to, builds an HTML\n",
    "    graph of that network. Optional parameters are width and height of graph.\n",
    "    Uses pyvis to build an interactive HTML graph of a NetworkX graph.\n",
    "    Uses NetworkX for graph storage.\n",
    "    \"\"\"\n",
    "    _net = Network(width=_width, height=_height, notebook=True)\n",
    "    _net.toggle_physics(_physics)\n",
    "    _net.barnes_hut()\n",
    "    _net.from_nx(_graph)\n",
    "    _net.show(_filename)\n",
    "\n",
    "def plot_top_words_one_topic(model, feature_names, n_top_words, title):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        plt.barh(top_features, weights, height=0.7)\n",
    "        plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "        plt.suptitle(title, fontsize=25)\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title, n_topics):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, n_topics, figsize=(15, 7), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded things\n",
    "\n",
    "columnDict = {0: \"hash\", 1: \"ngram\", 2: \"ngram_lc\", 3: \"ngram_num_tokens\", 4: \"ngram_count\", 5: \"term_freq\", 6: \"doc_count\"}\n",
    "# non_words = [\"a\", \"at\", \"an\", \"am\", \"and\", \"that\", \"like\", \"for\", \"by\", \"i\", \"in\", \"of\", \"or\", \"be\", \"use\", \"as\", \"on\", \"the\", \"to\", \"with\", \"-pron-\"]\n",
    "filenameAnte = \"data/doc_ngrams/sample.fgrep.antediluvian.txt\"\n",
    "filepathHennig = \"data/doc_ngrams/sample.fgrep.Hennig86.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-a9ef42162ea0>:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[newColumnName][n] = scaledSeries[n] + 1 # scales most (~98%) to be in range [0, 2] for graphing purposes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>ngram_lc</th>\n",
       "      <th>ngram_num_tokens</th>\n",
       "      <th>ngram_count</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_count</th>\n",
       "      <th>normalized_term_freq</th>\n",
       "      <th>ngram_lc_nounsAndVerbs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hash</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3002e8a37ec9d00a67bdf0004b8628c35d72068d</th>\n",
       "      <td>[antediluvian, antediluvian humanity]</td>\n",
       "      <td>[antediluvian, antediluvian humanity]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>0.345035</td>\n",
       "      <td>[antediluvian antediluvian humanity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005b3bf055ddcb3c25e4742a72ee16728934efd</th>\n",
       "      <td>[antediluvian, antediluvian refrain, follow by...</td>\n",
       "      <td>[antediluvian, antediluvian refrain, follow by...</td>\n",
       "      <td>[1, 2, 4, 5]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>0.995948</td>\n",
       "      <td>[antediluvian antediluvian refrain follow, an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005ebfe5508340797dbfcce8454f3d3f6f76eb1</th>\n",
       "      <td>[antediluvian, antediluvian dream, cave of -PR...</td>\n",
       "      <td>[antediluvian, antediluvian dream, cave of -pr...</td>\n",
       "      <td>[1, 2, 4, 5, 5]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>0.540615</td>\n",
       "      <td>[antediluvian antediluvian dream cave, -pron- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30064ae161de1e9a96992be108c195796f13e72a</th>\n",
       "      <td>[Hennig86 program, routine in the Hennig86, ro...</td>\n",
       "      <td>[hennig86 program, routine in the hennig86, ro...</td>\n",
       "      <td>[2, 4, 5, 1]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>0.797378</td>\n",
       "      <td>[hennig86 program routine, the hennig86 routin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30136ab3788ab8e8be6b939901ec669a41ef896a</th>\n",
       "      <td>[antediluvian]</td>\n",
       "      <td>[antediluvian]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.443641</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      ngram  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d              [antediluvian, antediluvian humanity]   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  [antediluvian, antediluvian refrain, follow by...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  [antediluvian, antediluvian dream, cave of -PR...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  [Hennig86 program, routine in the Hennig86, ro...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                     [antediluvian]   \n",
       "\n",
       "                                                                                   ngram_lc  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d              [antediluvian, antediluvian humanity]   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  [antediluvian, antediluvian refrain, follow by...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  [antediluvian, antediluvian dream, cave of -pr...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  [hennig86 program, routine in the hennig86, ro...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                     [antediluvian]   \n",
       "\n",
       "                                         ngram_num_tokens      ngram_count  \\\n",
       "hash                                                                         \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d           [1, 2]           [1, 1]   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd     [1, 2, 4, 5]     [1, 1, 1, 1]   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  [1, 2, 4, 5, 5]  [1, 1, 1, 1, 1]   \n",
       "30064ae161de1e9a96992be108c195796f13e72a     [2, 4, 5, 1]     [1, 1, 1, 1]   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a              [1]              [1]   \n",
       "\n",
       "                                          term_freq        doc_count  \\\n",
       "hash                                                                   \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d   0.000010           [1, 1]   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd   0.000281     [1, 1, 1, 1]   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1   0.000091  [1, 1, 1, 1, 1]   \n",
       "30064ae161de1e9a96992be108c195796f13e72a   0.000198     [1, 1, 1, 1]   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a   0.000051              [1]   \n",
       "\n",
       "                                          normalized_term_freq  \\\n",
       "hash                                                             \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d              0.345035   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd              0.995948   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1              0.540615   \n",
       "30064ae161de1e9a96992be108c195796f13e72a              0.797378   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a              0.443641   \n",
       "\n",
       "                                                                     ngram_lc_nounsAndVerbs  \n",
       "hash                                                                                         \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d               [antediluvian antediluvian humanity]  \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  [antediluvian antediluvian refrain follow, an ...  \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  [antediluvian antediluvian dream cave, -pron- ...  \n",
       "30064ae161de1e9a96992be108c195796f13e72a  [hennig86 program routine, the hennig86 routin...  \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                                 []  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test files into dataframe\n",
    "\n",
    "indexCol = columnDict[0]\n",
    "yakeScoreCol = columnDict[5]\n",
    "df_antedivulian = loadTextFileIntoDataframe(filepath=filenameAnte)\n",
    "df_hennig = loadTextFileIntoDataframe(filepath=filepathHennig)\n",
    "df = pd.concat([df_antedivulian, df_hennig])\n",
    "df.drop(7, axis=1, inplace=True)\n",
    "df.rename(columns=columnDict, inplace=True)\n",
    "df = df.groupby(indexCol).agg(list)\n",
    "\n",
    "for n in range(len(df)):\n",
    "    term_freq_list = df[yakeScoreCol].iloc[n]\n",
    "    df[yakeScoreCol][n] = term_freq_list[0]\n",
    "df[yakeScoreCol] = df[yakeScoreCol].astype(float)\n",
    "\n",
    "# df = getUniqueWordsColumn(df, \"ngram_lc\", \"ngram_words\", nonWords=non_words)\n",
    "df[\"normalized_term_freq\"] = np.zeros(len(df))\n",
    "standardizeDataColumn(df, yakeScoreCol, \"normalized_term_freq\")\n",
    "getNounsAndVerbs(df, \"ngram_lc\", \"ngram_lc_nounsAndVerbs\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "#\n",
    "#   DONE  - test adj list vs adj matrix storage\n",
    "#   DONE  - test adj list vs adj matrix compute performance\n",
    "#   DONE  - test building each with ngrams vs keywords\n",
    "# \n",
    "# - test performance and storage of scipy sparse matrix\n",
    "# - test partitioning with ngrams vs keywords\n",
    "# - test topic extraction with ngrams vs keywords\n",
    "# - test visualizing with ngrams vs keywords\n",
    "#\n",
    "# (figure out how to validate patitioning and topic extraction)\n",
    "# - test different community partition algos\n",
    "# - test different topic extraction algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import Timer\n",
    "funcList = [\"buildAdjacencyMatrixByColumn\", \"buildAdjacencyListByColumn\"]\n",
    "parameterList = [\"ngram_lc\"]\n",
    "numTests = 100\n",
    "\n",
    "testingDict = {}\n",
    "for func in funcList:\n",
    "    thisTest = {}\n",
    "    for param in parameterList:\n",
    "        funcString = func + \"(df, \\\"\" + param + \"\\\")\"\n",
    "        importString = \"from __main__ import \" + func + \" , df\"\n",
    "        t = Timer(funcString, importString)\n",
    "        testTime = t.timeit(number=numTests)\n",
    "        thisTest[\"compute time (seconds)\"] = testTime / numTests\n",
    "    testingDict[func] = thisTest\n",
    "\n",
    "testingDict[\"buildAdjacencyMatrixByColumn\"][\"storage (bytes)\"] = sys.getsizeof(buildAdjacencyMatrixByColumn(df, parameterList[0]))\n",
    "testingDict[\"buildAdjacencyListByColumn\"][\"storage (bytes)\"] = sys.getsizeof(buildAdjacencyListByColumn(df, parameterList[0]))\n",
    "\n",
    "testDF = pd.DataFrame(testingDict)\n",
    "testDF.to_csv(\"performance testing 1000 iterations.csv\")\n",
    "# storage in memory in bytes\n",
    "# compute time in seconds\n",
    "testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = Timer(\"buildAdjacencyMatrixByColumn(df, \\\"ngram_words\\\")\", \"from __main__ import buildAdjacencyMatrixByColumn, df\")\n",
    "# t.timeit(number=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Network and Community Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial community partition on all data\n",
    "\n",
    "# column to build adajency matrix \n",
    "adjColumnName = \"ngram_words\"\n",
    "\n",
    "print(\"Building Adjacency Matrix...\")\n",
    "t0 = time()\n",
    "adjMatrix = buildAdjacencyMatrixByColumn(df, adjColumnName)\n",
    "print(\"Done in %0.3fs.\\n\" % (time() - t0))\n",
    "\n",
    "print(\"Building Networkx Graph...\")\n",
    "t0 = time()\n",
    "G = nx.from_numpy_array(adjMatrix)\n",
    "print(\"Done in %0.3fs.\\n\" % (time() - t0))\n",
    "\n",
    "community_generator = nx.algorithms.community\n",
    "\n",
    "print(\"Building Louvain Partitions...\")\n",
    "t0 = time()\n",
    "community_sets = community_generator.louvain_communities(G, resolution=0.1)\n",
    "print(\"Done in %0.3fs.\\n\" % (time() - t0))\n",
    "\n",
    "\n",
    "numCommunities = 0\n",
    "numNodes = len(adjMatrix[0])\n",
    "nodes = list(range(numNodes))\n",
    "scalingFactor = 5. # scales the node size for visibility\n",
    "\n",
    "node_to_community = {}\n",
    "\n",
    "for set in community_sets:\n",
    "    for x in set:\n",
    "        node_to_community[x] = numCommunities\n",
    "    numCommunities = numCommunities + 1\n",
    "\n",
    "print(\"Adding Node Metadata...\")\n",
    "t0 = time()\n",
    "for n in nodes:\n",
    "    # 'title': hash (i.e. dataframe index)\n",
    "    # 'group': partition\n",
    "    # 'label': topic (empty for now)\n",
    "    # 'size': normalized YAKE score\n",
    "    G.nodes[n][\"title\"] = df.index[n]\n",
    "    G.nodes[n][\"group\"] = node_to_community[n]\n",
    "    G.nodes[n][\"size\"] = df[\"normalized_term_freq\"][n] * scalingFactor\n",
    "print(\"Done in %0.3fs.\\n\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nodes apart of sparse communities\n",
    "\n",
    "community_sets_pared = []\n",
    "X = 3\n",
    "\n",
    "for com in community_sets:\n",
    "    if (len(com) < X): # bypasses communities with fewer than X nodes\n",
    "        continue\n",
    "    else:\n",
    "        community_sets_pared.append(com)\n",
    "\n",
    "nodes_to_keep = list(itertools.chain(*community_sets_pared))\n",
    "\n",
    "print(\"Adjacency Matrix size before: \" + str(adjMatrix.shape))\n",
    "adjMatrixReduced = (adjMatrix[nodes_to_keep].T)[nodes_to_keep].T  # https://stackoverflow.com/questions/22927181/selecting-specific-rows-and-columns-from-numpy-array\n",
    "print(\"Adjacency Matrix size after: \" + str(adjMatrixReduced.shape) + \"\\n\")\n",
    "\n",
    "nodes_to_remove = subtractListsOfInts(nodes, nodes_to_keep)\n",
    "\n",
    "print(\"Removing Nodes...\")\n",
    "t0 = time()\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))\n",
    "numCommunitiesRemoved = numCommunities - len(community_sets_pared)\n",
    "print(\"Number of communities removed: \" + str(numCommunitiesRemoved))\n",
    "\n",
    "numCommunities = numCommunities - numCommunitiesRemoved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the whole data\n",
    "vectorizer = CountVectorizer()\n",
    "lda = LDA(n_components=numCommunities)\n",
    "\n",
    "doc = []\n",
    "\n",
    "print(\"Building Corpus...\")\n",
    "t0 = time()\n",
    "for hash in df.index:\n",
    "    hashDoc = df.loc[hash][adjColumnName]\n",
    "    doc.append(hashDoc)\n",
    "doc = list(itertools.chain(*doc)) # concatenates nested list into 1D list\n",
    "X = vectorizer.fit_transform(doc).toarray() # sparse matrix of token counts\n",
    "print(\"done in %0.3fs.\\n\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting Corpus...\")\n",
    "t0 = time()\n",
    "lda.fit(X)\n",
    "print(\"done in %0.3fs.\\n\" % (time() - t0))\n",
    "\n",
    "cols = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, cols, n_top_words=20, title=\"Topics in LDA model\", n_topics=numCommunities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on partitions of data\n",
    "\n",
    "def filterNode(n1):\n",
    "    return n1 in nodesInGroup\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "numTopics = 1 # number of topics to pull out for each community\n",
    "\n",
    "groupDict = nx.get_node_attributes(G, \"group\")\n",
    "groupList = list(Counter(list(nx.get_node_attributes(G, \"group\").values())))\n",
    "\n",
    "for community in groupList:\n",
    "\n",
    "    # get the community's nodes\n",
    "    nodesInGroup = [n for n in G.nodes if groupDict.get(n, \"group\")==community]\n",
    "    subgraph = nx.subgraph_view(G, filter_node=filterNode)\n",
    "    subgraphHashes = list(nx.get_node_attributes(subgraph, \"title\").values())\n",
    "\n",
    "    # build the doc\n",
    "    doc = []\n",
    "    \n",
    "    for hash in subgraphHashes:\n",
    "        hashDoc = df.loc[hash][adjColumnName]\n",
    "        doc.append(hashDoc)\n",
    "\n",
    "    # fit the model\n",
    "    doc = list(itertools.chain(*doc)) # concatenates nested list into 1D list\n",
    "    X = vectorizer.fit_transform(doc).toarray() # sparse matrix of token counts\n",
    "\n",
    "    lda = LDA(n_components=numTopics)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # add label to nodes\n",
    "    cols = vectorizer.get_feature_names_out()\n",
    "    feats = lda.components_[0]\n",
    "    featDict = dict(zip(cols, feats))\n",
    "    nodeLabel = max(featDict, key=featDict.get)\n",
    "    for n in nodesInGroup:\n",
    "        G.nodes()[n][\"label\"] = nodeLabel\n",
    "\n",
    "    # plot the topic\n",
    "    plot_top_words_one_topic(lda, cols, n_top_words=20, title=f\"Topics in LDA model for community: {community}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeaways\n",
    "#\n",
    "# for LDA and community generation, including all similarities in edge weights performs best.\n",
    "# for visualization, its incredibly slow.\n",
    "#\n",
    "# removing edges with weight=1 increases visualization runtime and readability, but \n",
    "# results in worse performing LDA and community generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeNetworkHTML(G, _filename=\"test labeling chaos.html\", _width=\"3840px\", _height=\"2160px\", _physics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74a87f91cde4fc93505a958135b0eb2fe5f761a6a4ac2799970d07df29216479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
